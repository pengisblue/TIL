# LangChain

## api키 설정 방법

- 방법 1. 터미널에서 실행 (터미널 종료시 사라짐)

```bash
export OPENAI_API_KEY="..."
```

- 방법 2. 코드에 api키 명시

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(openai_api_key="...")
```

## memory

> 대화를 기억하기위해 저장하는 방식

### LangChain에서 제공하는 메모리

#### Conversation Buffer

- 그동안의 모든 대화를 저장
- 단점: 대화가 길어질 수록 메모리 용량이 증가한다.

#### Conversation Buffer Window

- 마지막 k개의 대화만 저장
- 장점: 최근 대화만 저장하기에 메모리 용량이 과도하게 늘어나지 않는다.
- 단점: 이전 내용을 잊게 된다.

#### Conversation Summary

- LLM을 사용하여 대화 내용을 요약하며 저장

#### Conversation Summary Buffer

- max_token_limit을 초과할 경우에 요약하여 저장 (Buffer Window와 Summary의 결합)

#### Conversation Knowledge Graph

- 대화 속 주요 entity를 추출해 지식 그래프(knowledge graph)를 생성

### 파라미터

- llm: 요약을 하며 메모리에 저장하기에 llm이 필요하다.
- max_token_limit: 토큰 수가 80을 넘어가면 요약한다.
- memory_key: 메모리에 "chat_history"라는 키로 대화를 저장한다. (Default = history)
- return_messages:
  - True로 설정 시 각 메시지를 딕셔너리로 저장한 리스트를 반환한다.
  - False로 설정 시 string으로 반환한다. (Default = False)

## load_memory

- RunnablePassthrough을 사용하여 chain을 만들 때 메모리의 채팅 기록을 반환하는 함수

```python
def load_memory(input):
    return memory.load_memory_variables({})["chat_history"]
```

## Prompt

- 프롬프트를 생성할 때 '{}' 사이에 변수명을 넣어 사용
- format() 메서드
  - 변수명을 파라미터로 하고 내용을 값으로 하는 인자를 전달

### from_template()

- 템플릿을 str 타입으로 전달할 때 사용

```python
template = """
You are a helpful AI talking to human
{message}
"""
prompt = ChatPromptTemplate.from_template(template)

print(prompt.format(message="hi"))

# You are a helpful AI talking to human
# hi
```

### from_messages()

- 메시지 형태로 프롬프트를 작성할 때 사용
- 튜플 형태로 메시지를 전달한다.

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI talking to human"),
    ("human", "{question}"),
])
print(prompt.format(question="hi"))

# System: You are a helpful AI talking to human
# Human: hi
```

## MessagesPlaceholder()

- 대화 기록이 포함되는 곳
- 메모리의 chat_history키를 조회

```python
MessagesPlaceholder(variable_name="chat_history")
```

## Chain

- LangChain의 핵심 기능
- LCEL(LangChain Expression Language): 여러 컴포넌트들을 묶을 수 있는 기능
- 반환 값을 다음 컴포넌트의 입력 값으로 전달할 수 있도록 해 준다.
- LCEL은 streaming이나 비동기적 작동, 병렬 실행 등의 이점이 있다.

```python
chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm


def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context(
        {"input": question},
        {"output": result.content},
    )


invoke_chain("질문 내용")
```

### invoke_chain()

- chain을 실행하여 llm으로부터 결과 값을 받고 메모리에 저장하는 함수

### chain.invoke()

- chain을 호출하는 메서드
- 이 코드에서는 인자값으로 prompt에 전달될 변수명을 이름으로, 내용을 값으로 전달한다.
- llm의 반환값이 최종 반환값이 된다.

### memory.save_context()

- 메모리에 기록을 수동으로 직접 저장하는 메서드

## 체인의 흐름

```python
chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm
```

1. chain.invoke({"question": question})를 통해 사용자의 질문을 question키로 전달한다.
   - RunnablePassthrough(): 앞에서 전달받은 값을 변경 없이, 또는 다른 키를 추가하여 다음 컴포넌트로 그대로 전달한다. 이 코드에서는 chat_history키에 대한 값으로 load_memory를 호출한 반환값을 전달하였다.
2. prompt를 완성한다.
   - 위 프롬프트를 완성하기 위해서는 question과 chat_history 두 키가 필요하다.
3. 완성된 프롬프트가 llm로 전달되고, llm으로부터의 반환값이 최종 반환값이 된다.

### off-the-shelf chain

- 미리 만들어진 off-the-shelf chain을 사용한다면 더 쉽게 사용할 수 있다.
- 단, 이는 Legacy이므로, 앞선 LCEL로 직접 구현한 방식이 권장된다.

```python
# LCEL을 사용한 chain과 동일하게 작동한다.
# 메모리 저장 또한 자동으로 이루어진다.
# 간단하지만 작동 방식이 모호하고 커스텀하기 어렵다는 단점이 있다.

chain = LLMChain(
    llm=llm,
    memory=memory,
    prompt=prompt,
    verbose=True  # True로 설정 시 콘솔에 로그가 출력된다. 디버깅 시 사용된다.
)

chain.predict(question="질문 내용")
```
